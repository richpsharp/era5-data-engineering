Today:
* Got DB pipeline working manually
    * SOLUTION:
        1) I'm not able to use sparkcontext in Databricks, I have to spark sessions
        2) Need to use 'single user' nodes, otherwise so much sharing
            permissions, I had a problem with local_drive0 not being accessable
            related to shared, tried to set up init script but those don't run
            if you're 'shared', so i  run in single mode, which is really the
            era5-principal.
* What is the naming convention we should use?
    * I'm confused about using branches to define dev or staging or prod or whatever, what is it
        * Look at databricks.yaml, to me that is where 'dev' and 'prod' are defined? (https://github.com/richpsharp/era5-data-engineering/blob/refactor_source_to_staging_115/era5-daily/Combined_Run/databricks.yml)
            * Or else we pull dev/prod from the branch name? just seems like it's duplciated around
            * but then we also have two git workflows: https://github.com/richpsharp/era5-data-engineering/tree/refactor_source_to_staging_115/.github/workflows
                shouldn't there be 1 and it's picked from the branch?
    * I propose using _ instead of - whenever we name catalog/schema, handles
        better in database
    * {datasource}_{stage number}_{bronze/silver/gold}_{descriptive name}
        * i'm not sure about that bronze/silver/gold
* Runtimes:
    * initial pull, 22 minutes: https://dbc-ad3d47af-affb.cloud.databricks.com/jobs/804134530374442/runs/674897856681413?o=1488538362704485
    * daily update (3months check), 36 seconds: https://dbc-ad3d47af-affb.cloud.databricks.com/jobs/804134530374442/runs/297730054647134?o=148853836270448
* Parallelism -- using unity catalog and SparkSessions, not SparkContext, I am reliant on
    databricks doing all my scheduling. It breaks the jobs up into just however
    many workers i have.


* I saw a note that I should always prefer serverless?
* How can I get my logs from my workers? (look at logging: https://dbc-ad3d47af-affb.cloud.databricks.com/compute/clusters/0406-134207-et9icx79?o=1488538362704485)


* Get DB pipeline working via git actions? Or pull request first?

For later:
* Work on the pest trait pipeline
    * Chnage to stages so we have
        x pull searches
        x pull websites
        x process websites -- in progress
        x NLP part
            x scrubbing the websites so it just has relevant info in progress
            x need to run the LLM questions on the scrubbed websites
        * Fix issue of vote counting gone wrong
