Today:
* Get DB pipeline working manually
    * I am not able to get the injection into the table right, seems like a permission things
    * I think there's an issue with determining which catalog to reference,
        I think that should be a "job parameter" when running the job and
        if it doesn't exist default to sandbox. I otherwise have an error about
        whitlisting some permissions, i think for a table, but i don't know
        what table it is making.
    * SOLUTION:
        1) I'm not able to use sparkcontext in Databricks, I have to spark sessions
        2) Need to use 'single user' nodes, otherwise so much sharing
            permissions, I had a problem with local_drive0 not being accessable
            related to shared, tried to set up init script but those don't run
            if you're 'shared', so i  run in single mode, which is really the
            era5-principal.
* What is the naming convention we should use?
    * I'm confused about using branches to define dev or staging or prod or whatever, what is it
        * Look at databricks.yaml, to me that is where 'dev' and 'prod' are defined?
            * Or else we pull dev/prod from the branch name? just seems like it's duplciated around
    * I propose using _ instead of - whenever we name catalog/schema, handles
        better in database


* Get DB pipeline working via git actions? Or pull request first?

For later:
* Work on the pest trait pipeline
    * Chnage to stages so we have
        x pull searches
        x pull websites
        x process websites -- in progress
        x NLP part
            x scrubbing the websites so it just has relevant info in progress
            x need to run the LLM questions on the scrubbed websites
        * Fix issue of vote counting gone wrong
