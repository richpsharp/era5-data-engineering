{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "029601af-9e94-4440-beeb-e4a4cf27269a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Automated Ingestion and Processing of NetCDF Files using PySpark Autoloader\n",
    "\n",
    "This notebook outlines a comprehensive method for automatically loading, parsing, and processing NetCDF files into structured Spark tables. It employs PySpark's Autoloader feature for efficient data handling, supplemented by custom parsing functions to convert climate data stored in NetCDF format into analyzable Spark DataFrames. Each section is meticulously documented to provide clear guidance on setting up the environment, defining data schemas, and implementing data ingestion and enrichment workflows. This approach not only facilitates robust data management but also enhances the scalability and reliability of climate data analysis pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ef4886a-1bd6-4149-b643-8751e8a5ff59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Installation of Essential Libraries for Climate Data Handling\n",
    "\n",
    "This section of the notebook includes commands to install key Python libraries that are pivotal for handling and processing climate and scientific data:\n",
    "\n",
    "- `xarray`: This library is fundamental for working with and analyzing large multi-dimensional arrays of scientific data. It is particularly adept at handling netCDF files, which are commonly used for climate data.\n",
    "- `netCDF4` and `h5netcdf`: These libraries are crucial for interacting with netCDF and HDF5 data formats respectively. They provide the necessary tools to read, write, and manipulate datasets in these complex file formats.\n",
    "\n",
    "Executing these installation commands ensures that the Python environment is equipped with the necessary tools to handle the specific data types and operations discussed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3679bfa6-34d0-4d24-aff1-d9405bcac33b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xarray\n",
    "%pip install netCDF4 h5netcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3f5061c-1ae1-4314-ac97-2819959574ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Restarting Python Environment\n",
    "\n",
    "This command, `dbutils.library.restartPython()`, is used to restart the Python environment within Databricks notebooks. Restarting the Python environment is a critical step after installing new libraries or making significant changes to the environment. It ensures that all installed libraries are loaded correctly and that the environment is reset, clearing any residual state from previous computations. This operation is particularly useful when libraries that affect the entire runtime environment are added or updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccfb3058-4539-4922-8851-6930124014d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b6fbef9-0b43-4ec4-8b1e-76fa57a51bf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Importing Libraries and Modules for Data Processing\n",
    "\n",
    "This section includes the import statements necessary for data manipulation, file management, and structured data handling within the notebook. Each library or module plays a critical role in the data processing workflow:\n",
    "\n",
    "- `pandas` (imported as `pd`): Essential for data manipulation and analysis, particularly useful for handling tabular data with heterogeneously-typed columns.\n",
    "- `xarray` (imported as `xr`): Facilitates working with labeled multi-dimensional arrays and datasets, which is especially useful for manipulating large climate data files like netCDF.\n",
    "- `os`: Provides a way of using operating system dependent functionality like reading or writing to the filesystem, crucial for managing data files and directories.\n",
    "- `pyspark.sql.types`: This module includes classes that define the structure of DataFrames in PySpark, such as `StructType` and `StructField`, along with specific data types (`FloatType`, `StringType`, `TimestampType`, `LongType`, `BinaryType`). These are used to specify schema definitions for Spark DataFrames, ensuring data consistency and structure.\n",
    "- `datetime`: Used to handle and manipulate date and time data, which is crucial for time-series analysis and operations based on time conditions.\n",
    "\n",
    "By importing these modules and libraries, the notebook is equipped to handle a variety of data operations, from basic file interactions to complex data transformations within distributed environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d13d06-3486-41e3-a8eb-a95e8b0498b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os \n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, TimestampType, LongType, BinaryType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc0569dd-3c86-4099-8ecf-9697b21059cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Function: `netcdf_to_bronze_autoloader`\n",
    "\n",
    "This function facilitates the automated loading, parsing, and processing of NetCDF files into a structured Spark table using the PySpark Autoloader. It's designed to handle large datasets efficiently by leveraging Spark's distributed computing capabilities.\n",
    "\n",
    "#### Parameters:\n",
    "- **source_file_location (str):** The directory containing the source NetCDF files.\n",
    "- **output_schema (`StructType`):** The schema definition for the DataFrame to ensure consistency in data structure.\n",
    "- **checkpoint_location (str):** Path to store checkpoint information for fault tolerance.\n",
    "- **streaming_query_name (str):** Name for the streaming query to uniquely identify it.\n",
    "- **data_format (str):** The data format for reading files, typically set to \"BINARYFILE\" to delay parsing.\n",
    "- **table_name (str):** The name of the destination table where data will be written.\n",
    "- **schema_name (str):** The database schema name under which the table resides.\n",
    "- **catalog_name (str):** The name of the catalog in which the database schema is organized.\n",
    "- **write_mode (str):** Specifies the output mode of the stream (e.g., append, complete).\n",
    "- **data_provider (str):** Identifier for the source of the data, included as metadata.\n",
    "\n",
    "#### Workflow:\n",
    "1. **Loading the Stream:** Files are loaded as a binary stream using Spark's Autoloader, which ensures efficient handling of new or modified files.\n",
    "2. **Parsing NetCDF Files:** A custom function `parse_netcdf` is applied to each file to convert the data from NetCDF format to Pandas DataFrames, then to Spark DataFrames. This includes handling of specific data characteristics like experiment versions, and adding essential metadata such as file paths, timestamps, and source information.\n",
    "3. **Writing to Bronze Table:** The processed data is written to a Spark table defined by `bronze_table`, which is constructed dynamically from the catalog, schema, and table names provided. This step includes configurations for checkpointing and schema merging to ensure data integrity and flexibility in data management.\n",
    "\n",
    "#### Features:\n",
    "- **Efficient Data Ingestion:** Utilizes Spark's capability to handle large datasets and complex file operations.\n",
    "- **Metadata Management:** Enriches the dataset with metadata such as file creation dates and source information, enhancing data traceability and usability.\n",
    "- **Dynamic Schema Support:** Supports dynamic schema evolution, which allows for seamless integration of new data fields without disrupting existing processes.\n",
    "\n",
    "This function is integral for streamlining the ingestion and initial processing of climate data, setting a robust foundation for further analysis and insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b022396f-7daf-4ae7-a6d1-5fe6ced7d8a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def netcdf_to_bronze_autoloader(source_file_location,\n",
    "                     output_schema, checkpoint_location, \n",
    "                     streaming_query_name,data_format,table_name, \n",
    "                     schema_name, catalog_name,write_mode,\n",
    "                     data_provider,date_created_attr='date_created'): \n",
    "    \n",
    "   ## loading the stream \n",
    "    read_stream = (spark.readStream\n",
    "\t.format(\"cloudFiles\") # \"cloudFiles\" = use Autoloader\n",
    "\t.option(\"cloudFiles.format\", \"BINARYFILE\") # So we don't parse it until inside the foreachBatch\n",
    " \t.load(source_file_location) # Volume containing files. Can use *-wildcards\n",
    ") \n",
    "    \n",
    "\n",
    "    \n",
    "    ## a function to parse netcdf files and convert them into pandas and later spark dataframes\n",
    "    def parse_netcdf(iterator, source_file_path='Source_File_Path',\n",
    "                     ingest_timestamp_column='Ingest_Timestamp',\n",
    "                     data_provider= data_provider):  # Add default arguments for metadata\n",
    "        for pd_df in iterator:\n",
    "            for _, row in pd_df.iterrows():\n",
    "                xds = xr.open_dataset(row['path'])  # Assume row has a 'path' column\n",
    "\n",
    "                # Check if file contains ERA5T data\n",
    "                if 'expver' in xds.variables:\n",
    "                    # \"Experiment version\": if data older than 60 days, straightforward.\n",
    "                    # If newer, this flag distinguishes old and newer data.\n",
    "                    pdf = xds.sel(expver=1).drop_vars(['expver']).to_dataframe()\n",
    "                else:\n",
    "                    pdf = xds.to_dataframe() \n",
    "\n",
    "                pdf.dropna(inplace=True)  # Drop rows with null values\n",
    "\n",
    "                # Add metadata columns\n",
    "                pdf[source_file_path] = row['path']\n",
    "                pdf[ingest_timestamp_column] = pd.Timestamp.now()\n",
    "                pdf['data_provider'] = data_provider\n",
    "                pdf['source_file'] = xds.attrs.get('source_file', None)\n",
    "\n",
    "                # Convert file_modified_in_s3 to datetime\n",
    "                file_modified_in_s3 = xds.attrs.get('date_modified_in_s3', None)\n",
    "                if file_modified_in_s3:\n",
    "                    pdf['file_modified_in_s3'] = pd.to_datetime(file_modified_in_s3)\n",
    "                else:\n",
    "                    pdf['file_modified_in_s3'] = None\n",
    "\n",
    "                # Use the specified attribute to populate date_created, set to null if not present\n",
    "                file_creation_date = xds.attrs.get(date_created_attr, None)\n",
    "                if file_creation_date:\n",
    "                    pdf['date_created'] = pd.to_datetime(file_creation_date)\n",
    "                else:\n",
    "                    pdf['date_created'] = None\n",
    "\n",
    "\n",
    "\n",
    "                yield pdf.reset_index()  # Reset index to align time with variables  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    bronze_table = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "    autoload = (read_stream\n",
    "\t.selectExpr(\"_metadata.file_path as path\") # Keep only the file paths, as /Volumes/...\n",
    "  \t.repartition(64)\n",
    "\t.mapInPandas(parse_netcdf, output_schema) # Read each file and convert to Dataframe rows\n",
    "\t.writeStream # Writestream config must go below all the transformations\n",
    "\t.option(\"checkpointLocation\", checkpoint_location)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "\t.queryName(streaming_query_name)\n",
    "\t.outputMode(write_mode)\n",
    " \t.trigger(availableNow=True) # Runs on new files, then shuts down\n",
    "    .toTable(bronze_table))\n",
    "                \n",
    "\n",
    "  \n",
    "\n",
    "            \n",
    "\n",
    "                            \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "790a079d-c425-413f-b9a8-e7a41fa62a13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Define the expected netCDF data schema, so we can construct the Spark dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee7c9a4d-cc12-4da9-bbb9-7590ad7efa33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the expected netCDF data schema, so we can construct the Spark dataframe\n",
    "\n",
    "## decide what your output table might look like\n",
    "\n",
    "output_schema = StructType([\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"mean_t2m_c\", FloatType(), True),\n",
    "    StructField(\"max_t2m_c\", FloatType(), True),\n",
    "    StructField(\"min_t2m_c\", FloatType(), True),\n",
    "    StructField(\"sum_tp_mm\", FloatType(), True),\n",
    "    StructField(\"file_modified_in_s3\", TimestampType(), True),\n",
    "    StructField(\"source_file\", StringType(), True),\n",
    "    StructField(\"Source_File_Path\", StringType(), True),\n",
    "    StructField(\"Ingest_Timestamp\", TimestampType(), True),\n",
    "    StructField(\"data_provider\", StringType(), True),\n",
    "    StructField(\"date_created\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99b4c460-0ddb-49c1-b09d-cecded374d0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**IMPLEMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "420cadaa-cf04-470c-b41f-c6d84fb7b38c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "netcdf_to_bronze_autoloader(source_file_location = '/Volumes/pilot/bronze_test/era5_daily_summary/',\n",
    "                                output_schema = output_schema, checkpoint_location = '/Volumes/pilot/aer_era5_global_bronze/checkpoints/source_to_bronze_era5', \n",
    "                                streaming_query_name = 'Load ERA5 Files',\n",
    "                                data_format = 'delta',\n",
    "                                table_name = 'aer_era5_bronze_1940_to_present', \n",
    "                                schema_name = 'aer_era5_global_bronze', \n",
    "                                catalog_name = 'pilot',\n",
    "                                write_mode = 'append',\n",
    "                                data_provider='Atmospheric and Environmental Research (AER)',\n",
    "                                date_created_attr='date_updated')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "01_SM_HK_Source_to_Bronze_initial_ingest_autoloader",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
