{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39b7d65-67d5-439c-b933-0149bc5da1e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xarray netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0cea89-1b7a-4dca-b42e-50bf82bfddef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ace1d8-2ece-4a45-806d-52aad63cb7ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install psutil\n",
    "%restart_python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3004f716-a073-4023-93fc-46596737a2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import psutil\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "import threading\n",
    "import concurrent.futures\n",
    "\n",
    "SHOULD_STOP = False\n",
    "TARGET_DIR = '/local_disk0/era5'\n",
    "\n",
    "\n",
    "def copy_file(file_path):\n",
    "    shutil.copy(file_path, TARGET_DIR)\n",
    "    target_path = os.path.join(TARGET_DIR, file_path)\n",
    "    size = os.path.getsize(target_path)\n",
    "    try:\n",
    "        ds = xr.open_dataset(target_path)\n",
    "        ds.load()\n",
    "        ds.close()\n",
    "        return file_path, size, None\n",
    "    except Exception as exception:\n",
    "        try:\n",
    "            os.remove(target_path)\n",
    "        except:\n",
    "            pass\n",
    "        return file_path, size, exception\n",
    "\n",
    "\n",
    "def main():\n",
    "    max_workers = 1\n",
    "    offset = 365\n",
    "    index = 0\n",
    "    os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "    start = time.time()\n",
    "    era5_path_list = [\n",
    "        p for p in glob.glob('/Volumes/aer-processed/era5/daily_summary/*.nc')]\n",
    "    print(f'took {time.time()-start:.2f}s to read {len(era5_path_list)} files', flush=True)\n",
    "\n",
    "    batch_start = time.time()\n",
    "    total_bytes_copied = 0\n",
    "    total_completed = 0\n",
    "    bandwidth = 0\n",
    "    gbps = 0\n",
    "    approx_time_left = 0\n",
    "\n",
    "    print(f'max workers: {max_workers} -- ', end='', flush=True)\n",
    "\n",
    "    #monitor_thread = threading.Thread(target=monitor_cpu, daemon=True)\n",
    "    #monitor_thread.start()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        print('submitting copy commands', flush=True)\n",
    "        futures = [executor.submit(copy_file, fp) for fp in era5_path_list]\n",
    "\n",
    "        for f in concurrent.futures.as_completed(futures):\n",
    "            file_path, file_size, exception = f.result()\n",
    "            if exception:\n",
    "                print(f'error when copying {file_path}: {exception}')\n",
    "            total_completed += 1\n",
    "            total_bytes_copied += file_size\n",
    "            elapsed = time.time() - batch_start\n",
    "            files_left = len(era5_path_list) - total_completed\n",
    "            if elapsed > 0 and total_completed > 0:\n",
    "                bandwidth = (total_bytes_copied / (1024 * 1024)) / elapsed\n",
    "                gbps = (total_bytes_copied * 8 / (1024 * 1024 * 1024)) / elapsed\n",
    "                avg_time_per_file = elapsed / total_completed\n",
    "                approx_time_left = avg_time_per_file * files_left\n",
    "\n",
    "            print(\n",
    "                f'Files left: {files_left}, approx time left: {approx_time_left:.2f}s, bandwidth: {bandwidth:.2f} MB/s ({gbps:.2f} Gbps)', flush=True)\n",
    "\n",
    "    SHOULD_STOP = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a61bb94-1c4a-4011-9ad7-331c7c956cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_read = spark.table('experimental.my_schema.my_netcdf_blobs')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3538531902074848,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - Ingest Source",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
