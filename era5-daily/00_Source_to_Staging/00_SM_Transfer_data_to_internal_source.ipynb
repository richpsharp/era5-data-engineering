{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3074c1e2-6083-4002-81c8-9d0cf7fe774c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Handling and Processing Climate Data with PySpark and NetCDF\n",
    "\n",
    "This notebook demonstrates the techniques and processes involved in handling, analyzing, and managing large-scale climate data using PySpark, xarray, and netCDF4. It includes detailed explanations of how to set up the required Python environment, manage data files based on specific attributes such as date ranges, and efficiently process and move large datasets using distributed computing principles. Each section is thoroughly documented to ensure clarity and ease of understanding, facilitating the replication and adaptation of these methods for similar data-intensive tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c842bb39-2ade-4b0e-9161-0d1662faffcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Installation of Required Libraries\n",
    "\n",
    "This section includes the installation commands for Python libraries that are essential for handling and processing various data formats and performing parallel computations. Each library serves a specific purpose:\n",
    "\n",
    "- `xarray`: Used for labeling, indexing, and synchronizing multidimensional arrays, especially useful for working with climate data formats like netCDF.\n",
    "- `netCDF4` and `h5netcdf`: These libraries provide interfaces to netCDF and HDF5 files, respectively, allowing for efficient storage and access to large datasets.\n",
    "- `dask`: Enhances scalability and efficiency in analytics by enabling parallel computing.\n",
    "- `rioxarray`: Extends `xarray` to include tools for spatial analysis, such as rasterio integration for geospatial operations.\n",
    "- `tqdm`: Provides a progress bar for loops and other iterative computations, useful for tracking the progress of data processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc02aead-c752-4b82-a680-627b5909f751",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xarray\n",
    "%pip install netCDF4 h5netcdf\n",
    "%pip install dask   \n",
    "%pip install rioxarray\n",
    "%pip install tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "586dfe7e-ad6a-48e4-a2c1-f6c5da45038e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Restarting Python Environment\n",
    "\n",
    "This command, `dbutils.library.restartPython()`, is used to restart the Python environment within Databricks notebooks. Restarting the Python environment is a critical step after installing new libraries or making significant changes to the environment. It ensures that all installed libraries are loaded correctly and that the environment is reset, clearing any residual state from previous computations. This operation is particularly useful when libraries that affect the entire runtime environment are added or updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7283a43-9076-4e46-8a20-ba5fe87ef34e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf8d0f82-7097-4b29-a06b-ea4073c8d2d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Importing Necessary Libraries\n",
    "\n",
    "This code chunk imports the necessary libraries and modules required for data processing and analysis in a PySpark and scientific data context. Each import serves a specific function in the workflow:\n",
    "\n",
    "- `SparkSession`: Initializes a Spark session, which is the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "- `os`: Provides a way of using operating system dependent functionality like reading or writing to the filesystem.\n",
    "- `xarray` (imported as `xr`): Facilitates working with labeled multi-dimensional arrays and datasets, especially useful for manipulating large climate data files like netCDF.\n",
    "- `datetime`: Used to handle and manipulate date and time data, crucial for time-series analysis.\n",
    "- `shutil`: Offers high-level file operations such as copying and archiving.\n",
    "- `pandas` (imported as `pd`): Essential for data manipulation and analysis, particularly useful for handling tabular data with heterogeneously-typed columns.\n",
    "- `netCDF4` (imported as `nc`): Enables interaction with netCDF files which are commonly used for storing scientific data.\n",
    "- `lit`: A function from PySpark's SQL module that is used to add a new column with a constant value or to make explicit data type casting in DataFrame operations.\n",
    "- `tqdm.auto`: Automatically selects an appropriate progress bar based on the environment (notebook, terminal, etc.), useful for monitoring the progress of data processing loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3614b5b-0c08-46a5-b2af-7c1cdfc11242",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import pandas as pd \n",
    "import netCDF4 as nc\n",
    "from pyspark.sql.functions import lit \n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "428d4a5b-04ef-427f-8ab9-f8d578d3f303",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Function: `spark_copy_and_move_files_by_date`\n",
    "\n",
    "This function processes and moves NetCDF files from a source folder to a target folder based on a specified date range and file prefix. It leverages Apache Spark for distributed processing, which is particularly useful for handling large datasets efficiently.\n",
    "\n",
    "#### Parameters:\n",
    "- **start_date (str):** The start of the date range for filtering files. The date format is specified by `date_pattern`.\n",
    "- **end_date (str):** The end of the date range.\n",
    "- **source_folder (str):** The directory containing the source files.\n",
    "- **target_folder (str):** The destination directory for the processed files.\n",
    "- **prefix (str):** A prefix to filter files by name.\n",
    "- **date_pattern (str):** The format string for parsing dates in filenames (default is '%Y-%m-%d').\n",
    "- **source_file_attr (str):** The attribute name in the NetCDF file for the source file information (default is 'source_file').\n",
    "\n",
    "#### Returns:\n",
    "- None. The function outputs the process results directly, indicating successful moves and any issues encountered.\n",
    "\n",
    "#### Description:\n",
    "The function initializes a Spark session and then filters and processes files that match the given criteria (prefix and date range). Each file is processed using `xarray` for data manipulation, and metadata attributes are added or updated using `netCDF4`. Files are temporarily saved and then moved to the target folder. This process is parallelized using Spark's distributed computing capabilities to enhance performance, especially with large datasets. The function concludes by outputting the results of each file processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e693271-6068-476a-a3d3-b75790b7522e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def spark_copy_and_move_files_by_date(start_date, end_date, source_folder, target_folder, prefix, date_pattern='%Y-%m-%d', source_file_attr='source_file'):\n",
    "    \"\"\"\n",
    "    Process and move NetCDF files from one folder to another based on a date range and a prefix using Spark.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date (str): Start date in the format specified by date_pattern.\n",
    "    - end_date (str): End date in the format specified by date_pattern.\n",
    "    - source_folder (str): Path to the source folder containing the files.\n",
    "    - target_folder (str): Path to the target folder where the files will be moved.\n",
    "    - prefix (str): Prefix of the file names to consider.\n",
    "    - date_pattern (str): Date pattern in the filename (default: '%Y-%m-%d').\n",
    "    - source_file_attr (str): Attribute name for source file in the NetCDF metadata (default: 'source_file').\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"ProcessAndMoveFilesByDate\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Parse dates\n",
    "    start_date = datetime.strptime(start_date, date_pattern)\n",
    "    end_date = datetime.strptime(end_date, date_pattern)\n",
    "\n",
    "    # List all files in the source folder that match the prefix\n",
    "    all_files = [filename for filename in os.listdir(source_folder) if filename.startswith(prefix) and filename.endswith(\".nc\")]\n",
    "\n",
    "    # Initialize list for files within date range\n",
    "    filepaths_in_range = []\n",
    "\n",
    "    # For each file in the list, extract date and check if it's in the range\n",
    "    for filename in all_files:\n",
    "        # Replace underscores with hyphens in the date part of the filename\n",
    "        filename_with_hyphens = filename.replace('_', '-')\n",
    "        # Extract date from filename\n",
    "        date_str = filename_with_hyphens.split('-')[-3] + '-' + filename_with_hyphens.split('-')[-2] + '-' + filename_with_hyphens.split('-')[-1].split('.')[0]  # Assumes 'YYYY-MM-DD'\n",
    "        file_date = datetime.strptime(date_str, date_pattern)\n",
    "\n",
    "        # Check if the file date is within the range\n",
    "        if start_date <= file_date <= end_date:\n",
    "            filepath = os.path.join(source_folder, filename)\n",
    "            filepaths_in_range.append(filepath)\n",
    "\n",
    "    # Define a function to process and move each NetCDF file\n",
    "    def process_and_move_file(filepath):\n",
    "        # Process the file using xarray\n",
    "        ds = xr.open_dataset(filepath)\n",
    "\n",
    "        # Get the filename without the directory\n",
    "        filename = os.path.basename(filepath)\n",
    "\n",
    "        # Get the date_updated attribute from the dataset, set to null if not present\n",
    "        date_updated = ds.attrs.get('date_updated', None)\n",
    "\n",
    "        # Save the processed dataset to a temporary file in /tmp/\n",
    "        temp_file_path = os.path.join('/tmp/', filename)\n",
    "        ds.to_netcdf(temp_file_path)\n",
    "\n",
    "        # Get the modification time of the original file\n",
    "        date_modified_in_s3 = datetime.fromtimestamp(os.path.getmtime(filepath)).isoformat()\n",
    "\n",
    "        # Add date_updated, source file, and date_modified_in_s3 as metadata\n",
    "        with nc.Dataset(temp_file_path, 'a') as dst:\n",
    "            dst.setncattr('date_updated', str(date_updated) if date_updated is not None else 'null')\n",
    "            dst.setncattr(source_file_attr, filename)\n",
    "            dst.setncattr('date_modified_in_s3', date_modified_in_s3)\n",
    "\n",
    "        # Move the temporary file to the target directory\n",
    "        target_file_path = os.path.join(target_folder, filename)\n",
    "        shutil.move(temp_file_path, target_file_path)\n",
    "        return f\"Processed and moved {filename} to {target_folder}\"\n",
    "\n",
    "    # Distribute the file processing and moving using Spark\n",
    "    rdd = sc.parallelize(filepaths_in_range)\n",
    "    results = rdd.map(process_and_move_file).collect()\n",
    "\n",
    "    # Print results\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "    print(\"File processing and move complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4e95108-4a94-4d7c-8f6d-19be1e7815f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**IMPLEMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf5d8286-b429-4d92-8b77-f6b6553b5a1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "start_date = '1950-01-01'\n",
    "end_date =  '2023-12-31'\n",
    "source_folder = '/Volumes/aer-processed/era5/daily_summary'\n",
    "target_folder = '/Volumes/pilot/bronze_test/era5_daily_summary'\n",
    "prefix = 'reanalysis-era5-sfc-daily-'\n",
    "date_pattern='%Y-%m-%d'\n",
    "source_file_attr = 'source_file'\n",
    "\n",
    "spark_copy_and_move_files_by_date(start_date, \n",
    "                                  end_date, \n",
    "                                  source_folder, \n",
    "                                  target_folder, \n",
    "                                  prefix,\n",
    "                                  date_pattern,\n",
    "                                  source_file_attr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2700d0b2-4a3a-4228-9747-f7e4057d62c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_SM_Transfer_data_to_internal_source",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
